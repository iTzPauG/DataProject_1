# üíª DATA PROJECT: MONITORIZACI√ìN DE LA CALIDAD DEL AIRE

****

Este proyecto se basa en el dise√±o e implementaci√≥n de una **arquitectura moderna de Ingenier√≠a de Datos (Data Engineering)** para la ingesta, almacenamiento, transformaci√≥n y explotaci√≥n de datos de la calidad del aire procedentes de los portales de Datos Abiertos de Madrid y Valencia.

## 1. üéØ Presentaci√≥n y Objetivos del Proyecto

El Data Project simula un entorno de trabajo real, requiriendo la integraci√≥n de datos p√∫blicos, dise√±o arquitect√≥nico y toma de decisiones t√©cnicas para abordar la monitorizaci√≥n de la calidad del aire.

### Objetivos Clave

* **Integraci√≥n de Fuentes:** Integrar m√∫ltiples fuentes de datos heterog√©neas (APIs de Madrid y Valencia).
* **Transformaci√≥n y Calidad:** Homogeneizar, limpiar y transformar datos a trav√©s de procesos automatizados (dbt).
* **Arquitectura H√≠brida:** Garantizar una arquitectura escalable, modular y desacoplada (Kafka) para manejar **Streaming** y **Batch**.
* **Entrega BI:** Publicar los datos procesados para el an√°lisis a trav√©s de Tableau BI.
* **Mantenibilidad:** Dise√±ar un *pipeline* reproducible, mantenible y **versionado** con Git.

***

## 2. ‚öôÔ∏è Dise√±o de Arquitectura y Justificaci√≥n de Piezas

La arquitectura implementa un **sistema h√≠brido** que maneja el **an√°lisis hist√≥rico (Batch BI)** y el **monitoreo de baja latencia (Streaming)**, siguiendo el paradigma **ELT (Extract, Load, Transform)**.

### A. Capa de Ingesta y Fuente de Verdad

| Componente | Justificaci√≥n Estrat√©gica | Archivos de Evidencia |
| :--- | :--- | :--- |
| **APIs y API Gateway** | **Centraliza la seguridad y gesti√≥n de tr√°fico** (*rate limiting*). Se requieren *scripts* individualizados para fuentes heterog√©neas. | `ingestas/ingesta_madrid.py`, `ingestas/ingesta_valencia.py`, `api.py`. |
| **DB PROD (DuckDB)** | Act√∫a como destino inicial de la carga (la 'L' en ELT) y **Fuente de Verdad Operacional**. **DuckDB** se elige por su **excelente rendimiento en consultas anal√≠ticas** y f√°cil integraci√≥n con Docker, optimizando el costo. | `dev.duckdb`, `ingesta.py`. |

### B. Flujo de Eventos y Streaming (Tiempo Real) ‚ö°

| Componente | Justificaci√≥n Estrat√©gica Extendida | Archivos de Evidencia |
| :--- | :--- | :--- |
| **Bus de Eventos Kafka** | Es la **espina dorsal del *streaming***. Ofrece **desacoplamiento total**, **tolerancia a fallos** y capacidad para manejar **picos de alta concurrencia** (recibe "Nuevos Datos" v√≠a CDC). | `/kafka/docker-compose.yml`, `producer.py`, `kafka_consumer.py`. |
| **Dashboard de Alertas** | Componente dedicado a la **baja latencia**. Muestra **alertas casi instant√°neamente**, crucial para el monitoreo operacional. | `dashboard_alertas.py`. |

### C. Transformaci√≥n Anal√≠tica y Modelado (dbt) üõ†Ô∏è

| Componente | Justificaci√≥n Estrat√©gica Extendida | Archivos de Evidencia |
| :--- | :--- | :--- |
| **dbt (Data Build Tool)** | Implementa el paradigma **ELT**. Permite **versionar** el c√≥digo SQL en Git y realizar **pruebas automatizadas** de calidad de datos, garantizando la **confiabilidad** y **auditor√≠a** del dato. | Estructura `/dbt`, pruebas `unique_...sql`. |
| **Estructura en Capas** | Adopci√≥n del est√°ndar **Staging ‚Üí Intermediate ‚Üí Marts** para crear un **linaje de datos claro** y modular, optimizando la mantenibilidad. | Directorios `/staging`, `/intermediate`, `/marts`. |

## 3. üíæ Modelos de Datos: Dise√±o del Data Warehouse

El Data Warehouse de consumo se basa estrictamente en un conjunto de **Tablas de Hechos Pre-agregadas** (`mart_hourly` y `mart_monthly_promedio`) dise√±adas para la m√°xima velocidad de consulta en Tableau BI.

### A. Tablas de Hechos (Capa de Consumo Final)

Ambas tablas comparten la dimensi√≥n de **Estaci√≥n** (`city`, `nombre_estacion`).

#### 1. Tabla de Hechos Horaria: `mart_hourly`

Soporta an√°lisis de alta granularidad y la l√≥gica de clasificaci√≥n y ranking.

| Atributo | Rol Anal√≠tico | Definici√≥n / L√≥gica |
| :--- | :--- | :--- |
| **`fecha_hour`** | Dimensi√≥n | Granularidad horaria. |
| `no2_avg` a `pm25_avg` | M√©trica Base | Promedios de contaminantes por hora. |
| **`indice_contaminacion`** | M√©trica Calculada | √çndice de contaminaci√≥n por hora, promedio de los cuatro contaminantes. |
| **`nivel_no2`** | M√©trica Clasificada | Clasificaci√≥n de NO2 seg√∫n umbrales (ej. 'Muy Alto', 'Alto', 'Moderado', 'Bajo'). |
| **`ranking_pm25`** | M√©trica Calculada | Ranking de PM2.5 por hora dentro de cada ciudad, optimizado para el top N en BI. |

#### 2. Tabla de Hechos Mensual: `mart_monthly_promedio`

Soporta an√°lisis de tendencias a largo plazo y estacionalidad.

| Atributo | Rol Anal√≠tico | Definici√≥n / L√≥gica |
| :--- | :--- | :--- |
| **`fecha_month`** | Dimensi√≥n | Granularidad mensual. |
| `no2_avg` a `pm25_avg` | M√©trica Base | Promedios de contaminantes por mes. |
| **`contaminacion_promedio`** | M√©trica Calculada | Promedio general de los cuatro contaminantes para el mes. |

## 4. üîé Origen de Datos y Flujo de Entrega (BI)

### A. Datasets Explorados y Justificaci√≥n

### A. Datasets Explorados y Justificaci√≥n

| Dataset | Origen | Decisi√≥n | Justificaci√≥n | URL |
| :--- | :--- | :--- | :--- | :--- |
| **Calidad del Aire - Tiempo Real** | API Madrid | **INCLUIDO** | Proporciona los datos horarios necesarios para el **monitoreo en tiempo real**. | `https://ciudadesabiertas.madrid.es/dynamicAPI/API/query/calair_tiemporeal.json?pageSize=5000` |
| **Estaciones Contaminaci√≥n Atmosf√©rica** | API Valencia | **INCLUIDO** | Fuente de datos cr√≠tica para el an√°lisis geogr√°fico y la construcci√≥n de la **dimensi√≥n de la estaci√≥n**. | `https://valencia.opendatasoft.com/api/records/1.0/search/?dataset=estacions-contaminacio-atmosferiques-estaciones-contaminacion-atmosfericas&rows=1000` |


### B. Estructura del Repositorio Git y Entrega de BI üìä

| Carpeta / Archivo | Rol en la Arquitectura | Descripci√≥n / Instrucciones |
| :--- | :--- | :--- |
| **`README.md`** | **Instrucciones Principales** | Contiene el dise√±o de la arquitectura, la justificaci√≥n y los pasos de ejecuci√≥n. |
| **`requirements.txt`** | **Entorno Python** | Lista las librer√≠as necesarias para la reproducibilidad del entorno. |
| **`docker-compose.yml`** | **Infraestructura** | Define y orquesta los contenedores (Kafka, DB, Python/dbt). |
| **`/ingestas`** | Ingesta | Scripts de extracci√≥n de las APIs. |
| **`/kafka`** | Infraestructura Streaming | Configuraci√≥n de los servicios de Kafka y Zookeeper. |
| **`producer.py` / `kafka_consumer.py`** | Streaming | Scripts de env√≠o y recepci√≥n de datos por Kafka. |
| **`/dbt/models`** | Transformaci√≥n (ELT) | Contiene la l√≥gica SQL de modelado (`staging`, `intermediate`, `marts`). |
| **`pull_db_gsheets.py`** | Entrega BI | Script de *Reverse ETL* que extrae los *marts* para Tableau. |