# üí® DATA PROJECT I: Monitorizaci√≥n de la Calidad del Aire

**M√°ster en Big Data & Cloud 2025-2026**

**Tutor:** Pedro Nieto  
**Participantes:** Daniel Adam, Pau Garcia, Gemma Balaguer

## üåü 1. Introducci√≥n

Este proyecto implementa un **pipeline de datos moderno y escalable** para la monitorizaci√≥n en tiempo real de la calidad del aire de las ciudades de **Madrid y Valencia**.

El sistema integra la ingesta de APIs externas, almacenamiento relacional, procesamiento **Batch (dbt)** y **Streaming (Kafka)**, y entrega a m√∫ltiples herramientas de Business Intelligence (Plotly/Tableau), todo orquestado y empaquetado mediante **Docker Compose**.

### üõ†Ô∏è Stack Tecnol√≥gico Clave

| Categor√≠a | Tecnolog√≠a | Uso Principal |
| :--- | :--- | :--- |
| **Ingesta** | Python (Requests) | Extracci√≥n de datos de APIs de Ayuntamientos (Madrid y Valencia). |
| **Orquestaci√≥n** | Docker Compose | Definici√≥n, construcci√≥n y orquestaci√≥n de todos los servicios. |
| **Almacenamiento** | PostgreSQL | Base de datos OLTP/Anal√≠tica para datos brutos y transformados. |
| **Transformaci√≥n** | dbt (Data Build Tool) | Modelado de datos (ELT) con SQL para generar *data marts*. |
| **Streaming** | Apache Kafka | Plataforma de mensajer√≠a para desacoplamiento y procesamiento en tiempo real. |
| **Visualizaci√≥n** | Plotly / Tableau | Dashboards de baja latencia (Plotly) y an√°lisis profesional (Tableau). |

---

## üèóÔ∏è 2. Arquitectura del Proyecto

El sistema est√° dise√±ado como una arquitectura de **Lambda/Kappa simplificada**, combinando el procesamiento *batch* tradicional con capacidades de *streaming* en tiempo real.



### 2.1. Flujo de Datos

1.  **Ingesta (`/ingestas`):** Los scripts `ingesta_madrid.py` e `ingesta_valencia.py` extraen datos horarios de calidad del aire desde las APIs oficiales.
2.  **API Gateway (`api.py`):** Los datos ingestados se env√≠an a este *gateway* √∫nico para control de acceso, validaci√≥n y estandarizaci√≥n de formatos antes de la persistencia.
3.  **Base de Datos (`data_project_1` - PostgreSQL):** Almacenamiento centralizado de los datos **raw** (brutos).
4.  **Transformaciones (dbt):** Modelado de datos (limpieza, normalizaci√≥n y c√°lculo de m√©tricas) ejecutado sobre PostgreSQL para generar la capa de **Marts**.
5.  **Streaming (Kafka):**
    * **Producer (`producer.py`):** Lee continuamente nuevos registros de la base de datos y los env√≠a al *cluster* de Kafka.
    * **Consumer (`dashboard_consumer.py`):** Consume los mensajes de Kafka para alimentar un dashboard de alertas en **tiempo real** (Plotly).
6.  **Entrega BI (Reverse ETL):**
    * `pull_db_gsheets.py` exporta los *data marts* finales a Google Sheets.
    * Tableau consume la informaci√≥n desde Google Sheets/Drive para dashboards de **an√°lisis avanzado**.

---

## üöÄ 3. Ejecuci√≥n del Proyecto

El proyecto se despliega completamente mediante un √∫nico comando de `docker-compose`.

### 3.1. Requisitos

* **Docker Desktop** (o Docker Engine y Docker Compose) instalado y en ejecuci√≥n.
* Configuraci√≥n de credenciales de las APIs (si aplica) en el archivo `.env`.

### 3.2. Pasos de Ejecuci√≥n (Comando √önico)

El siguiente comando construye las im√°genes, arranca la infraestructura (PostgreSQL, Kafka, UIs) y ejecuta autom√°ticamente los *pipelines* de ingesta, transformaci√≥n y *streaming*.

```bash
docker compose up -d