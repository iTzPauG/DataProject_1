version: "3.9"

services:
  db:
    image: postgres:17-alpine
    container_name: db
    volumes:
      - postgresDB:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: data_project_1
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d data_project_1"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - kafka-net

  app:
    build: 
      context: .
      dockerfile: dockerfile
    pull_policy: build
    env_file: .env
    depends_on:
      db: 
        condition: service_healthy
    networks:
        - kafka-net
  ingestas:
    build:
      context: .
      dockerfile: dockerfile
    pull_policy: build
    container_name: ingesta
    env_file: .env
    depends_on:
      api:
        condition: service_started
    command: ["python", "ingesta.py"]
    restart: unless-stopped
    networks:
      - kafka-net
  dbt:
      build:
        context: .
        dockerfile: dockerfile
      pull_policy: build
      container_name: dbt
      volumes:
          - ./:/usr/app/dbt
          - ./profiles.yml:/root/.dbt/profiles.yml
      environment:
          - DBT_PROFILES_DIR=/usr/app/dbt
      working_dir: /usr/app/dbt/dbt_project
      entrypoint: ["/bin/bash"]
      tty: true
      depends_on:
          - db
      networks:
        - kafka-net

  zookeeper:
      image: confluentinc/cp-zookeeper:7.3.0
      container_name: zookeeper_air
      environment:
          ZOOKEEPER_CLIENT_PORT: 2181  # Puerto donde Zookeeper escucha
          ZOOKEEPER_TICK_TIME: 2000    # Intervalo de tiempo interno
      ports:
      - "2181:2181"                # Exponemos el puerto para acceso local
      networks:
      - kafka-net
  kafka:
      image: confluentinc/cp-kafka:7.3.0
      container_name: kafka_air
      depends_on:
        - zookeeper
      ports:
        - "9092:9092"                # Puerto para clientes locales
      environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    # Listener para aceptar conexiones en todas las interfaces
        KAFKA_LISTENERS: EXTERNAL://0.0.0.0:9092,INTERNAL://0.0.0.0:29092
    # Advertised listeners: cómo se anuncia el broker a los clientes
        KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:9092,INTERNAL://kafka:29092
    # Mapeo de protocolos
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
    # Listener interno para comunicación entre brokers
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
        KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      networks:
        - kafka-net
  api:
    build:
      context: .
      dockerfile: dockerfile
    pull_policy: build
    container_name: api
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "5001:5000"
    command: ["python", "api.py"]
    restart: unless-stopped
    networks:
      - kafka-net
  producer:
    build:
      context: .
      dockerfile: dockerfile
    pull_policy: build
    container_name: producer
    env_file: .env
    depends_on:
      db:
        condition: service_healthy
      kafka:
        condition: service_started
    command: ["python", "producer.py"]
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - default
      - kafka-net
    
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: pgesparterpubli@gmail.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: 'False'
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: 'False'
    ports:
      - "5050:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    depends_on:
      - db
    restart: unless-stopped
    networks:
      - kafka-net
  
  kafbat:
    image: ghcr.io/kafbat/kafka-ui:latest
    container_name: kafbat_ui
    ports:
      - "8080:8080"
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka
    restart: unless-stopped
    networks:
      - kafka-net
  
  consumer:
    build:
      context: .
      dockerfile: dockerfile
    pull_policy: build
    container_name: kafka_consumer
    env_file: .env
    depends_on:
      producer:
        condition: service_started
      kafka:
        condition: service_started
    command: ["python", "kafka_consumer.py"]
    restart: unless-stopped
    volumes:
      - shared_data:/shared
    networks:
      - kafka-net
<<<<<<< Updated upstream
=======
  
>>>>>>> Stashed changes
  dashboard:
    build:
      context: .
      dockerfile: dockerfile
    pull_policy: build
    container_name: dashboard_alertas
    env_file: .env
    depends_on:
      - consumer
    ports:
      - "8050:8050"
    command: ["python", "dashboard_alertas.py"]
    restart: unless-stopped
    volumes:
          - shared_data:/shared
    networks:
      - kafka-net

  docs:
    build:
      context: .
      dockerfile: Dockerfile.docs
    container_name: mkdocs
    ports:
      - "8000:8000"
    volumes:
      - ./docs:/usr/app/docs
    working_dir: /usr/app/docs
    restart: unless-stopped
    networks:
      - kafka-net

volumes:
    pg_data:
    postgresDB:
    pgadmin_data:
    shared_data:

networks:
  kafka-net:
    driver: bridge